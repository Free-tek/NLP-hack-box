{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download sherlock corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = !wget https://sherlock-holm.es/stories/plain-text/cnus.txt\n",
    "url = \"https://sherlock-holm.es/stories/plain-text/cnus.txt\"\n",
    "response = request.urlopen(url)\n",
    "text = response.read().decode('utf8')\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carry out case folding to solve irregular capitalization patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_casefolded = text.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "patn = '\\w+'\n",
    "text_tokenized_casefolded = regexp_tokenize(text_casefolded, patn)\n",
    "text_tokenized = regexp_tokenize(text, patn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'COMPLETE',\n",
       " 'SHERLOCK',\n",
       " 'HOLMES',\n",
       " 'Arthur',\n",
       " 'Conan',\n",
       " 'Doyle',\n",
       " 'Table',\n",
       " 'of',\n",
       " 'contents']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'complete',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'arthur',\n",
       " 'conan',\n",
       " 'doyle',\n",
       " 'table',\n",
       " 'of',\n",
       " 'contents']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized_casefolded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 17959 samples and 584678 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq_distribution_casefolded = FreqDist(text_tokenized_casefolded)\n",
    "print(freq_distribution_casefolded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER TO QUESTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: One of Sherlock Holmes' famous catch phrases is the use of the word 'undoubtedly'\n",
    "\n",
    "* How many times is the word 'undoubtedly' used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_distribution_casefolded['undoubtedly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'undoubtedly' was used 54 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER TO QUESTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characters are announced very deliberatly in the language of the setting in Victorian England. We can use this later to find characters in the book. But for now let's practice on a character we know\n",
    "\n",
    "How often is Sherlock Holmes refered to by 'Mr. Sherlock Holmes' vs 'Sherlock Holmes' vs. 'Mr. Holmes' vs 'Sherlock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sequence(lst, seq):\n",
    "     count = 0\n",
    "     len_seq = len(seq)\n",
    "     upper_bound = len(lst)-len_seq+1\n",
    "     for i in range(upper_bound):\n",
    "         if lst[i:i+len_seq] == seq:\n",
    "             count += 1\n",
    "     return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 21930 samples and 689379 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_distribution = FreqDist(tokenized_word)\n",
    "print(freq_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Sherlock Holmes: 0,  Sherlock Holmes: 361,  Mr. Holmes: 69, Sherlock: 383\n"
     ]
    }
   ],
   "source": [
    "print(f\" Mr. Sherlock Holmes: {count_sequence(tokenized_word, ['Mr', '.', 'Sherlock', 'Holmes'])},  Sherlock Holmes: {count_sequence(tokenized_word, ['Sherlock', 'Holmes'])},  Mr. Holmes: {count_sequence(tokenized_word, ['Mr', '.', 'Holmes'])}, Sherlock: {freq_distribution['Sherlock']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER TO QUESTION 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find all the doctors in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_list = []\n",
    "doctors = set(item_list)\n",
    "for i in range(len(text_tokenized)):\n",
    "    if text_tokenized[i] == \"Dr\":\n",
    "        item = text_tokenized[i+1]\n",
    "        if item not in doctors:\n",
    "            doctors.add(f\" {text_tokenized[i]} {item}\")\n",
    "            item_list.append(f\" {text_tokenized[i]} {item}\")\n",
    "    elif text_tokenized[i] == \"Doctor\":\n",
    "        a = f\"{text_tokenized[i+1]}\"\n",
    "        a = nlp(a)\n",
    "        for X in a.ents:\n",
    "            if(X.label_ ==\"ORG\"):\n",
    "                item = text_tokenized[i+1]\n",
    "                if item not in doctors:\n",
    "                    doctors.add(f\" {text_tokenized[i]} {item}\")\n",
    "                    item_list.append(f\" {text_tokenized[i]} {item}\")    \n",
    "            elif(X.label_ ==\"PERSON\"):\n",
    "                item = text_tokenized[i+1]\n",
    "                if item not in doctors:\n",
    "                    doctors.add(f\" {text_tokenized[i]} {item}\")\n",
    "                    item_list.append(f\" {text_tokenized[i]} {item}\")\n",
    "            elif(X.label_ ==\"NAME\"):\n",
    "                item = text_tokenized[i+1]\n",
    "                if item not in doctors:\n",
    "                    doctors.add(f\" {text_tokenized[i]} {item}\")\n",
    "                    item_list.append(f\" {text_tokenized[i]} {item}\")\n",
    "    elif text_tokenized[i] == \"doctor\":\n",
    "        a = f\"{text_tokenized[i+1]}\"\n",
    "        a = nlp(a)\n",
    "        for X in a.ents:\n",
    "            if(X.label_ ==\"ORG\"):\n",
    "                item = text_tokenized[i+1]\n",
    "                if item not in doctors:\n",
    "                    doctors.add(f\" {text_tokenized[i]} {item}\")\n",
    "                    item_list.append(f\" {text_tokenized[i]} {item}\")    \n",
    "            elif(X.label_ ==\"PERSON\"):\n",
    "                item = text_tokenized[i+1]\n",
    "                if item not in doctors:\n",
    "                    doctors.add(f\" {text_tokenized[i]} {item}\")\n",
    "                    item_list.append(f\" {text_tokenized[i]} {item}\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' Doctor Percy',\n",
       " ' Doctor Watson',\n",
       " ' Dr Ainstree',\n",
       " ' Dr Armstrong',\n",
       " ' Dr Barnicot',\n",
       " ' Dr Becher',\n",
       " ' Dr Ferrier',\n",
       " ' Dr Fordham',\n",
       " ' Dr Grimesby',\n",
       " ' Dr Horsom',\n",
       " ' Dr Huxtable',\n",
       " ' Dr James',\n",
       " ' Dr Leon',\n",
       " ' Dr Leslie',\n",
       " ' Dr Moore',\n",
       " ' Dr Mortimer',\n",
       " ' Dr Percy',\n",
       " ' Dr Richards',\n",
       " ' Dr Roylott',\n",
       " ' Dr Shlessinger',\n",
       " ' Dr Somerton',\n",
       " ' Dr Sterndale',\n",
       " ' Dr Thorneycroft',\n",
       " ' Dr Trevelyan',\n",
       " ' Dr Watson',\n",
       " ' Dr Willows',\n",
       " ' Dr Wood'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = []\n",
    "characters = set(item_list)\n",
    "\n",
    "for i in range(len(text_tokenized)):\n",
    "    skip = 0\n",
    "    a = f\"{text_tokenized[i]}\"\n",
    "    if a == \"Dr\" :\n",
    "        item = f\"{text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "        if item not in characters:\n",
    "            characters.add(item)\n",
    "            item_list.append(item)\n",
    "    elif a == \"Mr\":\n",
    "        item = f\"{text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "        if item not in characters:\n",
    "            characters.add(item)\n",
    "    elif a == \"Mrs\":\n",
    "        item = f\"{text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "        if item not in characters:\n",
    "            characters.add(item)\n",
    "    elif a == \"Miss\":\n",
    "        item = f\"{text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "        if item not in characters:\n",
    "            characters.add(item)\n",
    "    else:\n",
    "        a = nlp(a)\n",
    "        for X in a.ents:\n",
    "            if(X.label_ ==\"PERSON\"):\n",
    "                item = text_tokenized[i]\n",
    "                check = f\"{text_tokenized[i+1]}\"\n",
    "                check = nlp(check)\n",
    "                a = ([(X.label_) for X in check.ents])\n",
    "                if len(a) > 0:\n",
    "                    a = a[0]\n",
    "                    if a == \"ORG\":\n",
    "                        item = f\"{text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "                        if item not in characters:\n",
    "                            skip = 1\n",
    "                            characters.add(item)\n",
    "                            item_list.append(item)\n",
    "                    elif a == \"PERSON\":\n",
    "                        skip = 1\n",
    "                        item = f\"{text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "                        if item not in characters:\n",
    "                            characters.add(item)\n",
    "                            item_list.append(item)\n",
    "                if skip == 0:\n",
    "                    if item not in characters:\n",
    "                        characters.add(item)\n",
    "                        item_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_list = []\n",
    "doctors = set(item_list)\n",
    "month_list = ('January', 'Februray', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December')\n",
    "for i in range(len(text_tokenized)):\n",
    "    a = f\"{text_tokenized[i]}\"\n",
    "    a = nlp(a)\n",
    "    for X in a.ents:\n",
    "        if(X.label_ ==\"DATE\"):\n",
    "            if text_tokenized[i] != \"months\":\n",
    "                if text_tokenized[i] != \"weeks\":\n",
    "                    char = text_tokenized[i]\n",
    "                    if char[0].isdigit():\n",
    "                        doctors.add(char)\n",
    "                        item_list.append(char)\n",
    "                        print(f'{text_tokenized[i]}')\n",
    "                    elif text_tokenized[i] in month_list:\n",
    "                        date = text_tokenized[i]\n",
    "                        if text_tokenized[i-1] == \"of\":\n",
    "                            year_check = text_tokenized[i+1]\n",
    "                            if year_check[0].isdigit():\n",
    "                                char = f\"{text_tokenized[i-2]} {text_tokenized[i-1]} {text_tokenized[i]} {text_tokenized[i+1]}\"\n",
    "                                print(f\"{text_tokenized[i-2]} {text_tokenized[i-1]} {text_tokenized[i]} {text_tokenized[i+1]}\")\n",
    "                            else:\n",
    "                                char = f\"{text_tokenized[i-2]} {text_tokenized[i-1]} {text_tokenized[i]}\"\n",
    "                                doctors.add(char)\n",
    "                                item_list.append(char)\n",
    "                                print(f'{text_tokenized[i-2]} {text_tokenized[i-1]} {text_tokenized[i]}')\n",
    "                        else :\n",
    "                            print(f'{text_tokenized[i]}')\n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* capture all sentences that take about smoking (smoke, smokes, smoking, smoked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = []\n",
    "smoke = set(item_list)\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "for i in range(len(tokenized_sentence)):\n",
    "    tokenized_sent = f'{tokenized_sentence[i]}'\n",
    "    value1 = tokenized_sent.find(\"smoke\")\n",
    "    value2 = tokenized_sent.find(\"smokes\")\n",
    "    value3 = tokenized_sent.find(\"smoking\")\n",
    "    value4 = tokenized_sent.find(\"smoked\")\n",
    "    value5 = tokenized_sent.find(\"smoker\")\n",
    "    if value1 >= 0:\n",
    "        print(tokenized_sent)\n",
    "        smoke.add(tokenized_sent)\n",
    "        item_list.append(tokenized_sent)\n",
    "    elif value2 >= 0:\n",
    "        print(tokenized_sentI )\n",
    "        smoke.add(tokenized_sent)\n",
    "        item_list.append(tokenized_sent)\n",
    "    elif value3 >= 0:\n",
    "        print(tokenized_sent)\n",
    "        smoke.add(tokenized_sent)\n",
    "        item_list.append(tokenized_sent)\n",
    "    elif value4 >= 0:\n",
    "        print(tokenized_sent)\n",
    "        smoke.add(tokenized_sent)\n",
    "        item_list.append(tokenized_sent)\n",
    "    elif value5 >= 0:\n",
    "        print(*tokenized_sent)\n",
    "        smoke.add(tokenized_sent)\n",
    "        item_list.append(tokenized_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* capture the two words that appear after the smoking word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smoke_plus_2words = []\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "for i in range(len(tokenized_sentence)):\n",
    "    tokenized_sent = f'{tokenized_sentence[i]}'\n",
    "    word_token = tokenized_sent.split()\n",
    "    for j in range(len(word_token)):\n",
    "        if word_token[j] == \"smoke\":\n",
    "            smoke_plus_2words.append(f'{word_token[j]} {word_token[j+1]} {word_token[j+2]}')\n",
    "            print(word_token[j], word_token[j+1], word_token[j+2])\n",
    "        elif word_token[j] == \"smoked\":\n",
    "            smoke_plus_2words.append(f'{word_token[j]} {word_token[j+1]} {word_token[j+2]}')\n",
    "            print(word_token[j], word_token[j+1], word_token[j+2])\n",
    "        elif word_token[j] == \"smoking\":\n",
    "            smoke_plus_2words.append(f'{word_token[j]} {word_token[j+1]} {word_token[j+2]}')\n",
    "            print(word_token[j], word_token[j+1], word_token[j+2])\n",
    "        elif word_token[j] == \"smoked\":\n",
    "            smoke_plus_2words.append(f'{word_token[j]} {word_token[j+1]} {word_token[j+2]}')\n",
    "            print(word_token[j], word_token[j+1], word_token[j+2])\n",
    "        elif word_token[j] == \"smoker\":\n",
    "            smoke_plus_2words.append(f'{word_token[j]} {word_token[j+1]} {word_token[j+2]}')\n",
    "            print(word_token[j], word_token[j+1], word_token[j+2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_rev_2words = []\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "for i in range(1000):\n",
    "    tokenized_sent = f'{tokenized_sentence[i]}'\n",
    "    word_token = tokenized_sent.split()\n",
    "    for j in range(len(word_token)):\n",
    "        if word_token[j] == \"smoke\":\n",
    "            smoke_rev_2words.append(f'{word_token[j-2]} {word_token[j-1]} {word_token[j]}')\n",
    "            print(word_token[j-2], word_token[j-1], word_token[j])\n",
    "        elif word_token[j] == \"smoked\":\n",
    "            smoke_rev_2words.append(f'{word_token[j-2]} {word_token[j-1]} {word_token[j]}')\n",
    "            print(word_token[j-2], word_token[j-1], word_token[j])\n",
    "        elif word_token[j] == \"smoking\":\n",
    "            smoke_rev_2words.append(f'{word_token[j-2]} {word_token[j-1]} {word_token[j]}')\n",
    "            print(word_token[j-2], word_token[j-1], word_token[j])\n",
    "        elif word_token[j] == \"smoked\":\n",
    "            smoke_rev_2words.append(f'{word_token[j-2]} {word_token[j-1]} {word_token[j]}')\n",
    "            print(word_token[j-2], word_token[j-1], word_token[j])\n",
    "        elif word_token[j] == \"smoker\":\n",
    "            smoke_rev_2words.append(f'{word_token[j-2]} {word_token[j-1]} {word_token[j]}')\n",
    "            print(word_token[j-2], word_token[j-1], word_token[j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = \"Chapter2 This is Chapter1 and  is missing\"\n",
    "item = []\n",
    "import re\n",
    "item.append(re.split(\"Chapter\", book))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = []\n",
    "smoke = set(item_list)\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "for i in range(len(tokenized_sentence)):\n",
    "    tokenized_sent = f'{tokenized_sentence[i]}'\n",
    "    value1 = tokenized_sent.find(\"smoke\")\n",
    "    value2 = tokenized_sent.find(\"smokes\")\n",
    "    value3 = tokenized_sent.find(\"smoking\")\n",
    "    value4 = tokenized_sent.find(\"smoked\")\n",
    "    value5 = tokenized_sent.find(\"smoker\")\n",
    "    if value1 >= 0:\n",
    "        print(tokenized_sentence[i])\n",
    "        smoke.add(f\"{tokenized_sent[value1]} {tokenized_sent[value1 + 1]} {tokenized_sent[value1 + 2]}\")\n",
    "        item_list.append(f\"{tokenized_sent[value1]} {tokenized_sent[value1 + 1]} {tokenized_sent[value1 + 2]}\")\n",
    "    elif value2 >= 0:\n",
    "        print(tokenized_sent[value2], tokenized_sent[value2 + 1], tokenized_sent[value2 + 2])\n",
    "        smoke.add(f\"{tokenized_sent[value2]} {tokenized_sent[value2 + 1]} {tokenized_sent[value2 + 2]}\")\n",
    "        item_list.append(f\"{tokenized_sent[value2]} {tokenized_sent[value2 + 1]} {tokenized_sent[value2 + 2]}\")\n",
    "    elif value3 >= 0:\n",
    "        print(tokenized_sent[value3], tokenized_sent[value3 + 1], tokenized_sent[value3 + 2])\n",
    "        smoke.add(f\"{tokenized_sent[value3]} {tokenized_sent[value3 + 1]} {tokenized_sent[value3 + 2]}\")\n",
    "        item_list.append(f\"{tokenized_sent[value3]} {tokenized_sent[value3 + 1]} {tokenized_sent[value3 + 2]}\")\n",
    "    elif value4 >= 0:\n",
    "        print(tokenized_sent[value4], tokenized_sent[value4 + 1], tokenized_sent[value4 + 2])\n",
    "        smoke.add(f\"{tokenized_sent[value4]} {tokenized_sent[value4 + 1]} {tokenized_sent[value4 + 2]}\")\n",
    "        item_list.append(f\"{tokenized_sent[value4]} {tokenized_sent[value4 + 1]} {tokenized_sent[value4 + 2]}\")\n",
    "    elif value5 >= 0:\n",
    "        print(tokenized_sent[value5], tokenized_sent[value5 + 1], tokenized_sent[value5 + 2])\n",
    "        smoke.add(f\"{tokenized_sent[value5]} {tokenized_sent[value5 + 1]} {tokenized_sent[value5 + 2]}\")\n",
    "        item_list.append(f\"{tokenized_sent[value5]} {tokenized_sent[value5 + 1]} {tokenized_sent[value5 + 2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
